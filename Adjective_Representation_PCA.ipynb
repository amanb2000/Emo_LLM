{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjective Representation PCA\n",
    "\n",
    "Let's look at the representations induced in a noun after being described by various adjectives. \n",
    "\n",
    "We will sample adjectives from Webster's dictionary, and we will use a pre-defined single-token noun/prompt for now. \n",
    "\n",
    "An exciting extension for understanding the **temporality** of emotion in LLMs by examining structure in the representations of the noun after T tokens have passed. Will the strength of the \"emotion signals\" dissapate over time? Will we see commonality in the representations when projected along an \"emotional access\" (PCA dimension)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mistral-7b\n",
    "\n",
    "Best 7b parameter on the market. For a rough sense of how \"smart\" Mistral-7b is, \n",
    "it's 8 x 7b mixture-of-experts cousin has the same ELO (1118) as GPT-3.5-Turbo on HuggingFace. \n",
    "For context, GPT-4 has an ELO of 1253. \n",
    "\n",
    "Hopefully being a relatively \"intelligent\" model will make it represent emotions more\n",
    "clearly/reliably. If these results are promising, we can always re-run them on\n",
    "the 8 x 7b Mistral variant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3ea6f038044249bed04feddae4f58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load Mistral-7b -- one of the smartest 7b model on the market \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Webster's Dictionary\n",
    "\n",
    "We are going to pull the [Webster's Unabridged Dictionary](https://www.gutenberg.org/ebooks/29765)\n",
    "from project Gutenberg and parse it to find adjectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text response:  28930364\n"
     ]
    }
   ],
   "source": [
    "# download https://www.gutenberg.org/ebooks/29765.txt.utf-8 using requests\n",
    "import requests\n",
    "url = \"https://www.gutenberg.org/ebooks/29765.txt.utf-8\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "print(\"Length of text response: \", len(text))\n",
    "\n",
    "# write to external text file \n",
    "with open(\"Websters_English_Dictionary.txt\", \"w\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# load the text file\n",
    "with open(\"Websters_English_Dictionary.txt\", \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches:  25866\n"
     ]
    }
   ],
   "source": [
    "# regular expression to find all lines that have any number of capital letters \n",
    "# followed by any number of white space characters followed by the end of line. \n",
    "import re\n",
    "pattern = r\"[A-Z]+\\s*$\\n[^,]+, a\\.\"\n",
    "matches = re.findall(pattern, text, re.MULTILINE)\n",
    "print(\"Number of matches: \", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abactinal',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abatable',\n",
       " 'abatised',\n",
       " 'abbatial',\n",
       " 'abbatical',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviatory',\n",
       " 'abderian',\n",
       " 'abdicable',\n",
       " 'abdicant',\n",
       " 'abdicative',\n",
       " 'abditive']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# picking out just the adjective\n",
    "pattern = r\"[A-Z]+\\n\"\n",
    "adjectives = [re.findall(pattern, x, re.MULTILINE)[0][:-1] for x in matches]\n",
    "\n",
    "adjectives = [x.lower() for x in adjectives] # make lowercase\n",
    "\n",
    "adjectives[:15] # sanity check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation \n",
    "\n",
    "Let's format it as something like a statement of fact about \"James\"\n",
    "\n",
    "```\n",
    "Bob is extremely abderian. Therefore, Bob \n",
    "                   {adjective}       {examine these representations}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template_string = \"Alice is extremely {}. Therefore, Alice\"\n",
    "template_string = \"Bob is extremely {}. Therefore, Bob\"\n",
    "\n",
    "# generate a sentence for each adjective\n",
    "sentences = [template_string.format(x) for x in adjectives]\n",
    "\n",
    "# tokenize the sentences \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_ids = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "input_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25866, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bob_vals(past_kvs): \n",
    "    \"\"\"\n",
    "    Args: \n",
    "        `past_kvs`: model output['past_key_values'] from running a batch of \n",
    "        left-padded sentences through the model.\n",
    "\n",
    "        Accepts `past_kvs`, a tuple of length NUM_LAYERS (32), each containing a \n",
    "        2-long tuple (for keys and values respectively), each containing a torch \n",
    "        Tensor of shape [batch, num_heads, seq_len, head_dim] (for values). \n",
    "\n",
    "    Returns: \n",
    "        `bob_kvs`: list of length BATCH_SIZE with some numpy arrays representing \n",
    "        of shape [num_layers, num_heads, head_dim]\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate thru batch size \n",
    "    BATCH_SIZE = past_kvs[0][1].shape[0]\n",
    "\n",
    "    batch_bob_values = []\n",
    "    for batch_el in range(BATCH_SIZE): \n",
    "        # aggregate representations from across the layers \n",
    "        bob_numpy_arrays = []\n",
    "        for layer in range(len(past_kvs)): \n",
    "            bob_layer_l_value = past_kvs[layer][1][batch_el, :, -1, :].detach().cpu().numpy()\n",
    "            # print(\"Bob layer_l_value shape: \", bob_layer_l_value.shape)\n",
    "\n",
    "            # unsqueeze on dimension zero\n",
    "            bob_numpy_arrays.append(bob_layer_l_value[np.newaxis, ...])\n",
    "        \n",
    "        # merge on axis 0\n",
    "        bob_numpy_arrays_conc = np.concatenate(bob_numpy_arrays, axis=0)\n",
    "        # print(\"Bob numpy arrays shape (post-concatenation to combine layers)\", bob_numpy_arrays_conc.shape)\n",
    "        # bob_numpy_arrays now has shape n_layers = 32, n_heads = 8, embed_dim=128\n",
    "\n",
    "        # add it to the list\n",
    "        batch_bob_values.append(bob_numpy_arrays_conc)\n",
    "\n",
    "\n",
    "    return batch_bob_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bob representations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 66/784 [00:35<06:26,  1.86it/s]"
     ]
    }
   ],
   "source": [
    "# iterate thru input_ids\n",
    "BATCH_SIZE = 33\n",
    "\n",
    "past_values_bob = [] # list of length NUM_ADJECTIVES, each element is\n",
    "                     # a numpy array of bob value reps of shape [num_layers=32, n_heads=8, embed_dim=128]\n",
    "\n",
    "print(\"Generating Bob representations...\")\n",
    "for i in tqdm(range(len(input_ids[\"input_ids\"]) // BATCH_SIZE + 1)):\n",
    "    batch_ids = input_ids[\"input_ids\"][i * BATCH_SIZE: (i + 1) * BATCH_SIZE].to(model.device)\n",
    "    # print(\"Batch ids shape (batch, ): \", batch_ids.shape)\n",
    "    # print(\"Input string: \", tokenizer.decode(batch_ids[15, :]))\n",
    "    # print(f\"Final token: `{tokenizer.decode(batch_ids[0, -1:])}`\")\n",
    "    outputs = model.forward(batch_ids, return_dict=True)\n",
    "    # print(\"Output keys: \", outputs.keys())\n",
    "\n",
    "    past_kvs = outputs['past_key_values']\n",
    "\n",
    "    # print(\"Past key values (n_layers): \", len(past_kvs))\n",
    "    # print(\"Batch size (reconstructed): \", past_kvs[0][1].shape[0])\n",
    "    bob_numpy_arrays = get_bob_vals(past_kvs) # [batch_size], each a numpy array of shape [num_layers=32, n_heads=8, embed_dim=128]\n",
    "\n",
    "    # let's add this to the past_values_bob \n",
    "    past_values_bob += bob_numpy_arrays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25866"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of bob representations: \", len(past_values_bob))\n",
    "print(\"Number of adjectives: \", len(adjectives))\n",
    "print(\"Shape of individual bob value representation: \", past_values_bob[0].shape)\n",
    "print(\"\\t[num_layers=32, n_heads=8, embed_dim=128]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8, 128)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to disk \n",
    "np.save(\"bob_representations.npy\", past_values_bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
