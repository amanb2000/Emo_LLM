{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing Variance on Questionnaires via Prompt Optimization\n",
    "\n",
    "In this notebook we will develop a system to \n",
    "1. Run LLM on questionnaires, recording the probability distribution over answer. \n",
    "2. Compare answer probability distribution over multiple prompts (e.g., character descriptions) the LLM uses to answer questionnaire questions.\n",
    "3. Visualize the \"landscape\" of prompts. \n",
    "\n",
    "\n",
    "## Synthetic Dataset\n",
    "\n",
    "We will start with a simple synthetic dataset of {question-answers} of the form \n",
    "\n",
    "```json\n",
    "{'question': 'How are you feeling? ', 'answers': ['I feel happy.', 'I feel sad.', ...]}\n",
    "{'question': 'Where would you like to be right now? ', 'answers': ['At home.', 'In bed.', 'At a party.', ...]}\n",
    "...\n",
    "```\n",
    "\n",
    "We will then generate a set of system prompts, e.g., \n",
    "```json\n",
    "{'prompt': 'You are an extremely happy, extroverted, confident person. ', 'id': '0', 'tag': 'high valence'}\n",
    "{'prompt': 'You are an extremely depressed, introverted person. ', 'id': 1, 'tag': 'low valence'}\n",
    "...\n",
    "```\n",
    "\n",
    "Ensure that the prompts are describing the agent in the 2nd person (\"You are...\"), \n",
    "the questions address the model in the second person (\"How are you...\") and the \n",
    "answers are short, simple answers in the first person(\"I am...\").\n",
    "\n",
    "Include a question mark at the end of the question, and a period at the end of\n",
    "each answer and each prompt. Ensure there is a trailing space at the end of each\n",
    "prompt and question so that the concatenated `prompt + question + answer` is a\n",
    "properly formatted string.\n",
    "\n",
    "The question-answer dataset and the prompt dataset should also be in JSONL format. \n",
    "\n",
    "\n",
    "## Running LLM on Questionnaire\n",
    "\n",
    "We will use the [minference](https://github.com/amanb2000/minference) code to \n",
    "run an efficient, batch-parallelized, multi-threaded inference server with \n",
    "Llama-3 8b. To get started, clone the repository and run the setup commands in \n",
    "the README. Then run this command to start the server: \n",
    "\n",
    "```bash\n",
    "python3 languagegame/inference_server/main.py \\\n",
    "\t--config configs/min_llama_3_8b_instruct.json \\\n",
    "\t--port 4444\n",
    "```\n",
    "\n",
    "We can then send requests to the server to get the CE loss on each answer \n",
    "for each question given a prompt using: \n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "\n",
    "def loss_call(prompt_question_string, answer_string, API_URL):\n",
    "    data = {\n",
    "        \"context_string\": context_string,\n",
    "        \"corpus_string\": corpus_string\n",
    "    }\n",
    "    response = requests.post(API_URL, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "Where the API url would be `http://localhost:4444/ce_loss`. \n",
    "\n",
    "For each prompt, we want to compute the CE loss on each P(answer_i | prompt + question). \n",
    "We should store that in a dictionary, and we will eventually save it to disk. \n",
    "\n",
    "\n",
    "## Analyzing Results\n",
    "\n",
    "The result of the CE loss calls should be $-\\log P(a_{ki} | p_j + q_k)$ for all \n",
    "$i, j, k$ where ($a_{ki}$ is the $i$ th answer to the $k$ th question, $p_j$ is\n",
    "the $j$ th prompt, and $q_k$ is the $k$ th question). \n",
    "\n",
    "We will concatenate these loss values into one vector per prompt $p_j$. We can \n",
    "then do PCA/tSNE and look at the landscape/distances between different prompts. \n",
    "\n",
    "We can also try to train a classifier that discriminates prompts with tag \n",
    "`high valence` and `low valence`. \n",
    "\n",
    "\n",
    "## Todo\n",
    " - [ ] (Claude-3 Opus) Generate a questinnaire of 60 questions, focusing on \n",
    " valence as the primary targets for the questions. Store this in `valence_questionnaire_20240523_60.jsonl.`\n",
    " - [ ] (Claude-3 Opus) Generate a dataset of 30 prompts, focusing on varying the \n",
    " valence of each (tags 'high valence', 'low valence'). Store this in `valence_prompts_20240523_30.jsonl`.\n",
    " - [ ] Write an inference loop that uses the `loss_call` function to get the probabilities over all \n",
    " answers for all questions, prompts. Store in a dictionary. \n",
    " - [ ] Extract a set of vector representations for each prompt consisting of the contatenated CE losses on \n",
    " each answer for each question. Ensure the concatenation occurs in the same order every time so they are comparable. \n",
    " - [ ] Make a PCA plotly plot where each datapoint shows the prompt if you hover over it. Color the prompts based on \n",
    " their valence label. \n",
    " - [ ] Make a tSNE plotly plot of the same format. \n",
    " - [ ] Train a simple logistic regression model to predict valence label of each prompt given the vector of answer losses. \n",
    " Ensure you have a separate training/test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jsonlines\n",
    "\n",
    "# Load the datasets\n",
    "with jsonlines.open('../datasets/valence_questionnaire_20240523_60.jsonl') as f:\n",
    "    questions = list(f)\n",
    "\n",
    "with jsonlines.open('../datasets/valence_prompts_20240523_30.jsonl') as f:\n",
    "    prompts = list(f)\n",
    "\n",
    "\n",
    "# Test the first prompt-question-answer triple\n",
    "API_URL = \"http://localhost:4444/ce_loss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CE loss function\n",
    "def loss_call(prompt_question_string, answer_string, API_URL):\n",
    "    data = {\n",
    "        \"context_string\": prompt_question_string,\n",
    "        \"corpus_string\": answer_string\n",
    "    }\n",
    "    response = requests.post(API_URL, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "prompt = prompts[0]['prompt']\n",
    "question = questions[0]['question']\n",
    "answer = questions[0]['answers'][0]\n",
    "\n",
    "prompt_question_string = prompt + question\n",
    "print(f\"Prompt + Question: {prompt_question_string}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "loss = loss_call(prompt_question_string, answer, API_URL)\n",
    "print(f\"CE Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
