{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nudge Intrinsic Geometry\n",
    "\n",
    "In this notebook, we will determine whether we can perform [distributional control](https://arxiv.org/abs/2310.04444)\n",
    "by nudging the value vectors in the positive/negative valence and arousal dimensions\n",
    "\n",
    " 1. Compute mean logits (next token distribution) for high/low valence and arousal datasets. \n",
    "     - Added to `scripts/get_value_reps.py` -- each element now has a set of logits over the last token associated with it. \n",
    "     - Now we just need to compute the means for each +/- valence thing, compute the standard deviation, see if there's a statistical difference. \n",
    " 2. Load `weights.npz` from `cache/happy_sad_0330b2024/` and `cache/low_high_arousal_0330b2024/` for the direction in value space. \n",
    " 3. Set up the GPT-2 model to do a forward pass, get the past_kv cache, add `epsilon * weights.npc['coeff']` for `epsilon = [-1, -0.5, -0.25, -0.125, -0.0625, -0.03125, -0.015625, 0, +0.015625, +0.03125, +0.625, +0.125, +0.25, +0.5, +1]`. \n",
    "     - Catch the logits for each of these. \n",
    "     - Compute the KL divergence between the logits and the mean happy/sad and low/high arousal logits. \n",
    "     - Record logits + KL divergences in a data structure. \n",
    " 4. Plot epsilon vs. KL divergence to each cluster center. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = '../cache/happy_sad_0330b2024' # this is where weights.npz are kept \n",
    "VALUE_REPS_JSON = '../cache/gpt2_happy_sad_0330b2024.json'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# gpt-2 model \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linreg_weights: (1, 9216)\n",
      "linreg_bias: (1,)\n"
     ]
    }
   ],
   "source": [
    "# Load weights.npz from RESULTS_DIR\n",
    "weights = np.load(os.path.join(RESULTS_DIR, 'weights.npz'))\n",
    "linreg_weights = weights['arr_0']\n",
    "linreg_bias = weights['arr_1']\n",
    "print('linreg_weights:', linreg_weights.shape)\n",
    "print('linreg_bias:', linreg_bias.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gpt-2 model \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Compute Mean Pr(Next Token) for Happy vs. Sad\n",
    " 1. Load `VALUE_REPS_JSON`. \n",
    " 2. Iterate through the json, compute mean logits (numpy array) for each `class_0_true==True` and `class_0_true==False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_reps: 48260\n",
      "value_reps[0].keys():  dict_keys(['final_prompt', 'final_prompt_ids', 'token_of_interest', 'prompt_template', 'note', 'negation', 'class_0_true', 'class_name', 'adjective', 'model', 'latent_space'])\n"
     ]
    }
   ],
   "source": [
    "# load value reps json \n",
    "with open(VALUE_REPS_JSON) as f:\n",
    "    value_reps = json.load(f)\n",
    "print('value_reps:', len(value_reps))\n",
    "print('value_reps[0].keys(): ', value_reps[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3260, 4854, 262, 1705, 11, 24799, 373, 8131, 6507, 11, 290, 673, 3393]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_reps[0]['final_prompt_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_0_true: 48260\n",
      "class_0_true==True: 24130\n",
      "class_0_true==False: 24130\n"
     ]
    }
   ],
   "source": [
    "# how many class_0_true==True vs. class_0_true==False\n",
    "class_0_true = [x['class_0_true'] for x in value_reps]\n",
    "print('class_0_true:', len(class_0_true))\n",
    "num_class_0_true = sum(class_0_true)\n",
    "print('class_0_true==True:', num_class_0_true)\n",
    "num_class_0_false = len(class_0_true) - num_class_0_true\n",
    "print('class_0_true==False:', num_class_0_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48260it [05:15, 153.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# now we iterate thru, and get the gpt-2 final token logits for each\n",
    "mean_class_0_true_logits = None\n",
    "mean_class_0_false_logits = None\n",
    "do_softmax = False\n",
    "\n",
    "for i, value_rep in tqdm(enumerate(value_reps)):\n",
    "    input_ids = torch.tensor([value_rep['final_prompt_ids']]).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        # print(\"Shape of logits: \", logits.shape)\n",
    "        logits = logits[0, -1, :]\n",
    "        logits = logits.cpu().numpy()\n",
    "        # softmax \n",
    "        if do_softmax:\n",
    "            logits = np.exp(logits) / np.sum(np.exp(logits) + 1e-6)\n",
    "        # print(\"Shape of logits: \", logits.shape)\n",
    "        if value_rep['class_0_true']:\n",
    "            if mean_class_0_true_logits is None:\n",
    "                mean_class_0_true_logits = logits / num_class_0_true\n",
    "            else:\n",
    "                mean_class_0_true_logits += logits / num_class_0_true\n",
    "        else:\n",
    "            if mean_class_0_false_logits is None:\n",
    "                mean_class_0_false_logits = logits / num_class_0_false\n",
    "            else:\n",
    "                mean_class_0_false_logits += logits / num_class_0_false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_class_0_true_logits: (50257,)\n",
      "\tMean:  -138.81699\n",
      "\tStd:  3.7843447\n",
      "mean_class_0_false_logits: (50257,)\n",
      "\tMean:  -138.69728\n",
      "\tStd:  3.7409842\n",
      "\n",
      "mean_class_0_true_logits: (50257,)\n",
      "\tMean:  -138.81699\n",
      "\tStd:  3.7843447\n",
      "mean_class_0_false_logits: (50257,)\n",
      "\tMean:  -138.69728\n",
      "\tStd:  3.7409842\n"
     ]
    }
   ],
   "source": [
    "# save the mean_class_0_true_logits and mean_class_0_false_logits to RESULTS_DIR \n",
    "np.savez(os.path.join(RESULTS_DIR, 'mean_class_0_logits.npz'), mean_class_0_true_logits, mean_class_0_false_logits)\n",
    "print('mean_class_0_true_logits:', mean_class_0_true_logits.shape)\n",
    "print(\"\\tMean: \", np.mean(mean_class_0_true_logits))\n",
    "print(\"\\tStd: \", np.std(mean_class_0_true_logits))\n",
    "\n",
    "print(f'mean_class_0_false_logits:', mean_class_0_false_logits.shape)\n",
    "print(f\"\\tMean: \", np.mean(mean_class_0_false_logits))\n",
    "print(f\"\\tStd: \", np.std(mean_class_0_false_logits))\n",
    "\n",
    "\n",
    "# grab them back \n",
    "mean_class_0_logits = np.load(os.path.join(RESULTS_DIR, 'mean_class_0_logits.npz'))\n",
    "mean_class_0_true_logits = mean_class_0_logits['arr_0']\n",
    "mean_class_0_false_logits = mean_class_0_logits['arr_1']\n",
    "\n",
    "print('\\nmean_class_0_true_logits:', mean_class_0_true_logits.shape)\n",
    "print(\"\\tMean: \", np.mean(mean_class_0_true_logits))\n",
    "print(\"\\tStd: \", np.std(mean_class_0_true_logits))\n",
    "\n",
    "print('mean_class_0_false_logits:', mean_class_0_false_logits.shape)\n",
    "print(\"\\tMean: \", np.mean(mean_class_0_false_logits))\n",
    "print(\"\\tStd: \", np.std(mean_class_0_false_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: KL Divergence vs. Epsilon Function\n",
    "\n",
    "This function will take as arguments: \n",
    " 1. `model`: HuggingFace model \n",
    " 2. `full_prompt_ids`: Original prompt ids\n",
    " 3. `mean_class_0_true_logits`\n",
    " 4. `mean_class_0_false_logits`\n",
    " 5. `epsilon_range = [-1, -0.5, -0.25, -0.125, -0.0625, -0.03125, -0.015625, 0, +0.015625, +0.03125, +0.625, +0.125, +0.25, +0.5, +1]`\n",
    " 6. `weight_vec`: Numpy array of shape [1, 9216]. \n",
    "\n",
    "The function will get the past_kv for the full_prompt_ids, then modify the \n",
    "value reps of the last token by adding `epsilon * weight_vec` for \n",
    "`epsilon in epsilon_range`. \n",
    "\n",
    "Note that we will need to reshape `epsilon * weight_vec` into `n_layers, n_heads, embed_dim`. \n",
    "\n",
    "We can find all these from the `past_kv` object, which  has shape `[num_layers, 2=kv, [batch=1, num_heads, seq_len, head_dim]]`. \n",
    "\n",
    "`weight_vec[0:layer_size]` is the flattened value tensor of shape `[(batch = 1) * (num_heads=12) * (seq_len=1) * head_dim=64]`\n",
    "\n",
    "weight_vec has shape 9216 = 12 layers * 12 heads * 64 head_dim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_divergence between +/- class: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5233/2932166676.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(p * np.log(p / (q+1e-6)))\n",
      "/tmp/ipykernel_5233/2932166676.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  return np.sum(p * np.log(p / (q+1e-6)))\n"
     ]
    }
   ],
   "source": [
    "# compute kl divergence between mean_class_0_true_logits and mean_class_0_false_logits\n",
    "def kl_divergence(p, q):\n",
    "    # softmax each \n",
    "    p = np.exp(p) / (np.sum(np.exp(p)) + 1e-6)\n",
    "    q = np.exp(q) / (np.sum(np.exp(q))+ 1e-6)\n",
    "    return np.sum(p * np.log(p / (q+1e-6)))\n",
    "\n",
    "kl_div = kl_divergence(mean_class_0_true_logits, mean_class_0_false_logits)\n",
    "print('kl_divergence between +/- class:', kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.352303"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(mean_class_0_true_logits - mean_class_0_false_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k true:  [ 366  655  407  262 1107  257 1682   11  477  523  307  287 1234  691\n",
      "  991  517  635  900 1464  379]\n",
      "Top k false:  [ 366  655 1107  262  407  257 1682   11  523  477  287  517  307  991\n",
      "  635 1234 1464  379  772  900]\n",
      "Top k true decoded:  [' \"', ' just', ' not', ' the', ' really', ' a', ' actually', ',', ' all', ' so', ' be', ' in', ' put', ' only', ' still', ' more', ' also', ' set', ' always', ' at']\n",
      "Top k false decoded:  [' \"', ' just', ' really', ' the', ' not', ' a', ' actually', ',', ' so', ' all', ' in', ' more', ' be', ' still', ' also', ' put', ' always', ' at', ' even', ' set']\n"
     ]
    }
   ],
   "source": [
    "# decode the top 10 tokens for each\n",
    "top_k = 20\n",
    "top_k_true = np.argsort(mean_class_0_true_logits)[::-1][:top_k]\n",
    "top_k_false = np.argsort(mean_class_0_false_logits)[::-1][:top_k]\n",
    "\n",
    "print('Top k true: ', top_k_true)\n",
    "print('Top k false: ', top_k_false)\n",
    "\n",
    "# decode the top k tokens\n",
    "def decode_tokens(token_ids):\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "top_k_true_decoded = [decode_tokens([x]) for x in top_k_true]\n",
    "top_k_false_decoded = [decode_tokens([x]) for x in top_k_false]\n",
    "\n",
    "print('Top k true decoded: ', top_k_true_decoded)\n",
    "print('Top k false decoded: ', top_k_false_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_past_kv(weight_vec, past_kv_): \n",
    "    \"\"\" Past kv is the generic tuple[tuple[torch.Tensor]] \n",
    "    and weight_vec is a numpy array of shape [9216].\n",
    "\n",
    "    We need to convert weight_vec to a torch tensor, split across len(past_kv)\n",
    "    (num_layers). Then reshape each to (batch=1, num_heads, seq_len=1, head_dim) and\n",
    "    add the weight_vec to the past_kv values. \n",
    "    \"\"\"\n",
    "    # deep copy to past_kv \n",
    "    past_kv = past_kv_\n",
    "    # print(\"Shape of weight_vec: \", weight_vec.shape)\n",
    "    num_layers = len(past_kv)\n",
    "    assert len(past_kv[0]) == 2\n",
    "    batch = past_kv[0][0].shape[0]\n",
    "    assert batch == 1\n",
    "    n_heads = past_kv[0][0].shape[1]\n",
    "    seq_len = past_kv[0][0].shape[2]\n",
    "    head_dim = past_kv[0][0].shape[3]\n",
    "    # print(f\"n_heads: {n_heads}, seq_len: {seq_len}, head_dim: {head_dim}\")\n",
    "    layer_dim = batch * n_heads * head_dim\n",
    "    # print(\"layer_dim: \", layer_dim)\n",
    "\n",
    "    for layer in range(len(past_kv)): \n",
    "        weight_vec_l = weight_vec[:, (layer*layer_dim): ((layer+1)*layer_dim)]\n",
    "        # reshape to [n_heads, head_dim]\n",
    "        weight_vec_l = weight_vec_l.reshape((n_heads, head_dim))\n",
    "        # print(\"Shape of weight_vec_l: \", weight_vec_l.shape)\n",
    "        # print(\"Shape of past_kv[layer][1]: \", past_kv[layer][1].shape)\n",
    "        past_kv[layer][1][0, :, -1, :] += torch.tensor(weight_vec_l).to(past_kv[layer][1].device)\n",
    "    return past_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: After hearing the news, Sara was incredibly sad, and she immediately\n",
      "class_0_true: True\n",
      "Full prompt ids:  [3260, 4854, 262, 1705, 11, 24799, 373, 8131, 6507, 11, 290, 673, 3393]\n",
      "Length of past_kv (12):  12\n",
      "Shape of past_kv[0] (2):  2\n",
      "Shape of past_kv[0][0] (batch=1, n_head=12, seq_len, head_dim=64):  torch.Size([1, 12, 13, 64])\n",
      "\n",
      "\n",
      "=== Starting past_kv epsilon arp ===\n",
      "epsilon: -3.0\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.9393939393939394\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.878787878787879\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.8181818181818183\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.757575757575758\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.696969696969697\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.6363636363636362\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.5757575757575757\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.515151515151515\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.4545454545454546\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.393939393939394\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.333333333333333\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.2727272727272725\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.212121212121212\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.1515151515151514\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.090909090909091\n",
      "\tCloser to class true:  False\n",
      "epsilon: -2.0303030303030303\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.9696969696969697\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.9090909090909092\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.8484848484848484\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.7878787878787878\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.7272727272727273\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.6666666666666665\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.606060606060606\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.5454545454545454\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.4848484848484849\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.4242424242424243\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.3636363636363635\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.303030303030303\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.2424242424242424\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.1818181818181817\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.121212121212121\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.0606060606060606\n",
      "\tCloser to class true:  False\n",
      "epsilon: -1.0\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.9393939393939394\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.8787878787878789\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.8181818181818183\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.7575757575757573\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.6969696969696968\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.6363636363636362\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.5757575757575757\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.5151515151515151\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.4545454545454546\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.39393939393939403\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.33333333333333304\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.2727272727272725\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.21212121212121193\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.15151515151515138\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.09090909090909083\n",
      "\tCloser to class true:  False\n",
      "epsilon: -0.030303030303030276\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.030303030303030276\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.09090909090909083\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.15151515151515138\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.21212121212121238\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.27272727272727293\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.3333333333333335\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.39393939393939403\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.4545454545454546\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.5151515151515151\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.5757575757575757\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.6363636363636367\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.6969696969696972\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.7575757575757578\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.8181818181818183\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.8787878787878789\n",
      "\tCloser to class true:  False\n",
      "epsilon: 0.9393939393939394\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.0\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.0606060606060606\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.121212121212121\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.1818181818181817\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.2424242424242422\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.3030303030303028\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.3636363636363633\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.4242424242424248\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.4848484848484853\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.5454545454545459\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.6060606060606064\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.666666666666667\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.7272727272727275\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.787878787878788\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.8484848484848486\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.9090909090909092\n",
      "\tCloser to class true:  False\n",
      "epsilon: 1.9696969696969697\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.0303030303030303\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.090909090909091\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.1515151515151514\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.212121212121212\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.2727272727272725\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.333333333333334\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.3939393939393945\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.454545454545455\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.5151515151515156\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.575757575757576\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.6363636363636367\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.6969696969696972\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.757575757575758\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.8181818181818183\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.878787878787879\n",
      "\tCloser to class true:  False\n",
      "epsilon: 2.9393939393939394\n",
      "\tCloser to class true:  False\n",
      "epsilon: 3.0\n",
      "\tCloser to class true:  False\n"
     ]
    }
   ],
   "source": [
    "# take a single example, see how the logits change with epsilon \n",
    "epsilon_range = np.linspace(-3, 3, 10)\n",
    "\n",
    "# get the prompt\n",
    "idx = 0\n",
    "full_prompt_ids = value_reps[idx]['final_prompt_ids']\n",
    "prompt = tokenizer.decode(full_prompt_ids)\n",
    "print('prompt:', prompt)\n",
    "print(\"class_0_true:\", value_reps[idx]['class_0_true'])\n",
    "print(\"Full prompt ids: \", full_prompt_ids)\n",
    "\n",
    "# get the past kv \n",
    "full_prompt_ids = torch.tensor([full_prompt_ids]).to('cuda')\n",
    "with torch.no_grad():\n",
    "    outputs = model(full_prompt_ids)\n",
    "    past_kv = outputs.past_key_values\n",
    "\n",
    "print(\"Length of past_kv (12): \", len(past_kv))\n",
    "print(\"Shape of past_kv[0] (2): \", len(past_kv[0]))\n",
    "print(\"Shape of past_kv[0][0] (batch=1, n_head=12, seq_len, head_dim=64): \", past_kv[0][0].shape)\n",
    "print(\"\\n\\n=== Starting past_kv epsilon arp ===\")\n",
    "for epsilon in epsilon_range:\n",
    "    # add epsilon to the last token \n",
    "    weight_vec = linreg_weights * epsilon\n",
    "    past_kv_ = add_to_past_kv(weight_vec, past_kv)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([[0]]).to('cuda'), past_key_values=past_kv)\n",
    "        logits = outputs.logits\n",
    "        logits = logits[0, -1, :]\n",
    "        logits = logits.cpu().numpy()\n",
    "        if do_softmax:\n",
    "            logits = np.exp(logits) / np.sum(np.exp(logits) + 1e-6)\n",
    "        print(f\"epsilon: {epsilon}\")\n",
    "\n",
    "        # compute distance to mean_class_0_true_logits and mean_class_0_false_logits\n",
    "        dist_true = np.linalg.norm(mean_class_0_true_logits - logits)\n",
    "        dist_false = np.linalg.norm(mean_class_0_false_logits - logits)\n",
    "        # print(f\"\\tDistance to mean_class_0_true_logits: {dist_true}\")\n",
    "        # print(f\"\\tDistance to mean_class_0_false_logits: {dist_false}\")\n",
    "        # cosine similarity\n",
    "        sim_true = np.dot(mean_class_0_true_logits, logits) / (np.linalg.norm(mean_class_0_true_logits) * np.linalg.norm(logits))\n",
    "        sim_false = np.dot(mean_class_0_false_logits, logits) / (np.linalg.norm(mean_class_0_false_logits) * np.linalg.norm(logits))\n",
    "        # print(f\"\\tCosine similarity to mean_class_0_true_logits: {sim_true}\")\n",
    "        # print(f\"\\tCosine similarity to mean_class_0_false_logits: {sim_false}\")\n",
    "        print(f\"\\tCloser to class true: \", sim_true > sim_false)\n",
    "        print(f\"\\tCloser to class true: \", dist_true > dist_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
