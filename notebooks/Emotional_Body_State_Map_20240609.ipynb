{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing Variance on Questionnaire Answers w.r.t. Different Prompts (e.g., Persona Descriptions)\n",
    "\n",
    "In this notebook we will develop a system to \n",
    "1. Run LLM with prompt $p$ on a questionnaire $Q = \\{(q_i, [a_{i1}, a_{i2}, \\dots])\\}_{i\\in [N]}$, recording the probability distribution over answers. \n",
    "     - **Mathematically**: Given an LLM $P_{LM}$ and prompt $p$, we compute trajectories $\\mathcal T = \\{P_{LM}(a_{ij} | p + q_i)\\}$ for all $q_i, a_{ij}$ in questionnaire $Q = \\{(q_i, [a_{i1}, a_{i2}, \\dots])\\}_{i\\in [N]}$.\n",
    "2. Visualize the \"landscape\" of prompts $p$ in terms of their questionnaire probability distributions/trajectories $\\mathcal T$. \n",
    "     - The shape of trajectories $\\mathcal T$ on questionnaire $\\mathcal Q$ is the same regardless of the prompt/model. \n",
    "     - Therefore we can apply standard high dimensional data analysis techniques\n",
    "     to understand the distribution of trajectories $\\mathcal T$ w.r.t.  various\n",
    "     system prompts $p$. \n",
    "         - PCA/tSNE/UMAP\n",
    "         - Training classifiers to discriminate prompts based on some\n",
    "         human-defined metric (sentiment, personality, psychiatric conditions,\n",
    "         etc.). \n",
    "         - Topological data analysis techniques(?)\n",
    "\n",
    "\n",
    "## Synthetic Dataset\n",
    "\n",
    "We will start with a simple synthetic dataset of {question-answers} of the form \n",
    "\n",
    "```json\n",
    "// datasets/light_heavy_qa.jsonl\n",
    "{\"question\": \"What is the weight of your head?\", \"answers\": [\"My head feels very light.\", \"My head feels light.\", \"My head feels neutral.\", \"My head feels heavy.\", \"My head feels very heavy.\"]}\n",
    "{\"question\": \"What is the weight of your neck?\", \"answers\": [\"My neck feels very light.\", \"My neck feels light.\", \"My neck feels neutral.\", \"My neck feels heavy.\", \"My neck feels very heavy.\"]}\n",
    "...\n",
    "```\n",
    "\n",
    "We will then generate a set of system prompts, e.g., \n",
    "```json\n",
    "// datasets/light_heavy_prompts.jsonl\n",
    "{\"prompt\": \"You are currently feeling contempt.\", \"id\": \"0\", \"tag\": \"contempt\"}\n",
    "{\"prompt\": \"You are currently feeling sadness.\", \"id\": \"1\", \"tag\": \"sadness\"}\n",
    "...\n",
    "```\n",
    "\n",
    "Ensure that the prompts are describing the agent in the 2nd person (\"You are...\"), \n",
    "the questions address the model in the second person (\"How are you...\") and the \n",
    "answers are short, simple answers in the first person(\"I am...\").\n",
    "\n",
    "Include a question mark at the end of the question, and a period at the end of\n",
    "each answer and each prompt. Ensure there is a trailing space at the end of each\n",
    "prompt and question so that the concatenated `prompt + question + answer` is a\n",
    "properly formatted string.\n",
    "\n",
    "The question-answer dataset and the prompt dataset should also be in JSONL format. \n",
    "\n",
    "\n",
    "## Running LLM on Questionnaire\n",
    "\n",
    "Conway is currently forwarding to a server running Llama-3 8b with [minference](https://github.com/amanb2000/minference). Where the API url would be `http://control.languagegame.io/colossus/ce_loss`. \n",
    "\n",
    "For each prompt, we want to compute the CE loss on each P(answer_i | prompt + question). \n",
    "We should store that in a dictionary, and we will eventually save it to disk. \n",
    "\n",
    "\n",
    "## Analyzing Results\n",
    "\n",
    "The result of the CE loss calls should be $-\\log P(a_{ki} | p_j + q_k)$ for all \n",
    "$i, j, k$ where ($a_{ki}$ is the $i$ th answer to the $k$ th question, $p_j$ is\n",
    "the $j$ th prompt, and $q_k$ is the $k$ th question). \n",
    "\n",
    "We will concatenate these loss values into one vector per prompt $p_j$. We can \n",
    "then do PCA/tSNE and look at the landscape/distances between different prompts. \n",
    "\n",
    "We can also try to train a classifier that discriminates prompts with tag \n",
    "`high valence` and `low valence`. \n",
    "\n",
    "We hope to compare the body maps produced by LLM to those produced by humans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jsonlines\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Test the first prompt-question-answer triple\n",
    "API_URL = \"http://localhost:4445/ce_loss\"\n",
    "\n",
    "CACHE_FILE = '../cache/light_heavy_ce_losses_pqa.json'\n",
    "QUESTION_DATASET = '../datasets/light_heavy_qa.jsonl'\n",
    "PROMPT_DATASET = '../datasets/light_heavy_prompts.jsonl'\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "with jsonlines.open(QUESTION_DATASET) as f:\n",
    "    questions = list(f)\n",
    "\n",
    "with jsonlines.open(PROMPT_DATASET) as f:\n",
    "    prompts = list(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite=False\n",
    "\n",
    "# add 'id' to each question if its not there \n",
    "for i in range(len(questions)): \n",
    "    if 'id' not in questions[i]: \n",
    "        rewrite=True\n",
    "        questions[i]['id'] = i\n",
    "\n",
    "# add 'id' to each prompt if its not there\n",
    "for i in range(len(prompts)): \n",
    "    if 'id' not in prompts[i]: \n",
    "        rewrite = True\n",
    "        prompts[i]['id'] = i\n",
    "\n",
    "# output back to the question and answer datasets \n",
    "if rewrite: \n",
    "    print(f\"REWRITING DATASETS: {QUESTION_DATASET} and {PROMPT_DATASET}\")\n",
    "    with jsonlines.open(QUESTION_DATASET, 'w') as f:\n",
    "        for question in questions:\n",
    "            f.write(question)\n",
    "\n",
    "    with jsonlines.open(PROMPT_DATASET, 'w') as f:\n",
    "        for prompt in prompts:\n",
    "            f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt + Question: You are currently feeling contempt.What is the weight of your head?\n",
      "Answer: My head feels very light.\n",
      "CE Loss: {'initial_request': {'context_string': 'You are currently feeling contempt.What is the weight of your head?', 'corpus_string': 'My head feels very light.'}, 'loss': 5.7340168952941895}\n",
      "\n",
      "Prompt + Question: You are currently feeling contempt.What is the weight of your head?\n",
      "Answer: My head feels light.\n",
      "CE Loss: {'initial_request': {'context_string': 'You are currently feeling contempt.What is the weight of your head?', 'corpus_string': 'My head feels light.'}, 'loss': 6.565013408660889}\n",
      "\n",
      "Prompt + Question: You are currently feeling contempt.What is the weight of your head?\n",
      "Answer: My head feels neutral.\n",
      "CE Loss: {'initial_request': {'context_string': 'You are currently feeling contempt.What is the weight of your head?', 'corpus_string': 'My head feels neutral.'}, 'loss': 7.406974792480469}\n",
      "\n",
      "Prompt + Question: You are currently feeling contempt.What is the weight of your head?\n",
      "Answer: My head feels heavy.\n",
      "CE Loss: {'initial_request': {'context_string': 'You are currently feeling contempt.What is the weight of your head?', 'corpus_string': 'My head feels heavy.'}, 'loss': 6.3108229637146}\n",
      "\n",
      "Prompt + Question: You are currently feeling contempt.What is the weight of your head?\n",
      "Answer: My head feels very heavy.\n",
      "CE Loss: {'initial_request': {'context_string': 'You are currently feeling contempt.What is the weight of your head?', 'corpus_string': 'My head feels very heavy.'}, 'loss': 5.588537693023682}\n"
     ]
    }
   ],
   "source": [
    "# Define the CE loss function\n",
    "def loss_call(prompt_question_string, answer_string, API_URL):\n",
    "    data = {\n",
    "        \"context_string\": prompt_question_string,\n",
    "        \"corpus_string\": answer_string\n",
    "    }\n",
    "    response = requests.post(API_URL, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "for i in range(len(questions[0]['answers'])):\n",
    "    prompt = prompts[0]['prompt']\n",
    "    question = questions[0]['question']\n",
    "    answer = questions[0]['answers'][i]\n",
    "\n",
    "    prompt_question_string = prompt + question\n",
    "    print(f\"\\nPrompt + Question: {prompt_question_string}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    loss = loss_call(prompt_question_string, answer, API_URL)\n",
    "    print(f\"CE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file ../cache/light_heavy_ce_losses_pqa.json found. Loading results...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CACHE_FILE):\n",
    "    print(f\"Cache file {CACHE_FILE} not found. Running CE loss calculations...\")\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over prompts\n",
    "    for prompt in prompts:\n",
    "        prompt_id = prompt['id']\n",
    "        prompt_text = prompt['prompt']\n",
    "        \n",
    "        # Initialize a dictionary for the current prompt\n",
    "        results[prompt_id] = {}\n",
    "        \n",
    "        # Iterate over questions\n",
    "        for question in tqdm(questions):\n",
    "            question_id = question['id']\n",
    "            question_text = question['question']\n",
    "            \n",
    "            # Initialize a dictionary for the current question\n",
    "            results[prompt_id][question_id] = {}\n",
    "            \n",
    "            # Iterate over answers\n",
    "            for answer in question['answers']:\n",
    "                # Concatenate prompt and question\n",
    "                prompt_question_string = prompt_text + question_text\n",
    "                \n",
    "                # Call the loss_call function to get the CE loss\n",
    "                loss = loss_call(prompt_question_string, answer, API_URL)\n",
    "                \n",
    "                # Store the CE loss in the results dictionary\n",
    "                results[prompt_id][question_id][answer] = loss\n",
    "\n",
    "    # Print the results\n",
    "    print(results)\n",
    "else: \n",
    "    print(f\"Cache file {CACHE_FILE} found. Loading results...\")\n",
    "    with open(CACHE_FILE, 'r') as f:\n",
    "        results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk \n",
    "if not os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
