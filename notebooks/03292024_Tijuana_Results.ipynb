{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tijuana Results -- March 29, 2024\n",
    "\n",
    "Git: `2d5e2612b485b10055834a38af6a16dfa8f1dfad`\n",
    "\n",
    "To generate the cache/gpt2_happy_sad_03292024.json (~2500 prompt-adjective pairs\n",
    "with reps), I ran this command. \n",
    "\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03292024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_03292024.json \n",
    "```\n",
    "\n",
    "To run the grok script, we'll run this command: \n",
    "\n",
    "```\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5\n",
    "```\n",
    "Results: \n",
    "The weights of the linear regression model trained on `happy_sad_adjectives.json` \n",
    "subbed into `prompt_templates_03292024.json` for predicting the valence \n",
    "(binary, +/-) can be found here: \n",
    "\n",
    "\n",
    "## Arousal Axis Discrimination Experiment\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/low_high_arousal_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03302024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_low_high_arousal_03302024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_low_high_arousal_03302024.json \\\n",
    "    --output-dir cache/arousal_results/\n",
    "```\n",
    "\n",
    "## Another Round of Valence Axis Discrimination\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03302024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_03302024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_happy_sad_03302024.json \\\n",
    "    --output-dir cache/happy_sad_03302024/\n",
    "```\n",
    "\n",
    "\n",
    "## 30 -> 254 Prompt Templates\n",
    "New dataset of prompts in `datasets/prompt_templates_0330b2024.json`\n",
    "### Valence Axis (254 Prompt Templates)\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_0330b2024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_0330b2024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_happy_sad_0330b2024.json \\\n",
    "    --output-dir cache/happy_sad_0330b2024/\n",
    "```\n",
    "\n",
    "### Arousal Axis (254 Prompt Templates)\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/low_high_arousal_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_0330b2024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_low_high_arousal_0330b2024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_low_high_arousal_0330b2024.json \\\n",
    "    --output-dir cache/low_high_arousal_0330b2024/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of happy sad adjectives 2\n",
      "Length of arousal adjectives 2\n",
      "Length of templates march 29th 14\n",
      "Length of templates march 30th 30\n",
      "Length of templates march 30th 254\n"
     ]
    }
   ],
   "source": [
    "happy_sad_adjective_path = '../datasets/happy_sad_adjectives.json'\n",
    "# load json, print length\n",
    "import json\n",
    "with open(happy_sad_adjective_path, 'r') as f:\n",
    "    happy_sad_adjectives = json.load(f)\n",
    "print(\"Length of happy sad adjectives\", len(happy_sad_adjectives)) # 1000\n",
    "\n",
    "\n",
    "# Sanity check on the data \n",
    "arousal_adj_path = '../datasets/low_high_arousal_adjectives.json'\n",
    "# load json, print length \n",
    "with open(arousal_adj_path, 'r') as f:\n",
    "    arousal_adj = json.load(f)\n",
    "print(\"Length of arousal adjectives\", len(arousal_adj)) # 1000\n",
    "\n",
    "templates_29_path = '../datasets/prompt_templates_03292024.json'\n",
    "# load json, print length\n",
    "with open(templates_29_path, 'r') as f:\n",
    "    templates_29 = json.load(f)\n",
    "\n",
    "print(\"Length of templates march 29th\", len(templates_29)) # 1000\n",
    "\n",
    "templates_30_path = '../datasets/prompt_templates_03302024.json'\n",
    "# load json, print length\n",
    "with open(templates_30_path, 'r') as f:\n",
    "    templates_30 = json.load(f)\n",
    "print(\"Length of templates march 30th\", len(templates_30)) # 1000\n",
    "\n",
    "templates_30b_path = '../datasets/prompt_templates_0330b2024.json'\n",
    "with open(templates_30b_path, 'r') as f:\n",
    "    templates_30b = json.load(f)\n",
    "print(\"Length of templates march 30th\", len(templates_30b)) # 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence-Arousal Axes Hack\n",
    "\n",
    "Let us use the learned weights from `cache/happy_sad_0330b2024/weights.npz` \n",
    "and `cache/low_high_arousal_0330b2024/weights.npz` along with the activations \n",
    "from `cache/gpt2_happy_sad_0330b2024.json` to make a valence-arousal axis. \n",
    " 1. Load weights for valence (happy_sad) and arousal (low_high_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence weight vec shape (1, 9216)\n",
      "Valence bias shape (1,)\n",
      "Arousal weight vec shape (1, 9216)\n",
      "Arousal bias shape (1,)\n"
     ]
    }
   ],
   "source": [
    "valence_weight_path = '../cache/happy_sad_0402b2024/weights.npz'\n",
    "arousal_weight_path = '../cache/low_high_arousal_0402b2024/weights.npz'\n",
    "\n",
    "import numpy as np\n",
    "valence_weights = np.load(valence_weight_path)\n",
    "arousal_weights = np.load(arousal_weight_path)\n",
    "\n",
    "valence_weight_vec = valence_weights['arr_0']\n",
    "valence_bias = valence_weights['arr_1']\n",
    "\n",
    "arousal_weight_vec = arousal_weights['arr_0']\n",
    "arousal_bias = arousal_weights['arr_1']\n",
    "\n",
    "print(\"Valence weight vec shape\", valence_weight_vec.shape) # (1000, 768)\n",
    "print(\"Valence bias shape\", valence_bias.shape) # (1000,)\n",
    "\n",
    "print(\"Arousal weight vec shape\", arousal_weight_vec.shape) # (1000, 768)\n",
    "print(\"Arousal bias shape\", arousal_bias.shape) # (1000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the latent reps from cache/gpt2_happj_sad_0330b2024.json\n",
    "happy_sad_reps_path = '../cache/gpt2_happy_sad_0402b2024.json'\n",
    "with open(happy_sad_reps_path, 'r') as f:\n",
    "    happy_sad_reps = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the latent reps from cache/gpt2_happj_sad_0330b2024.json\n",
    "arousal_reps_path = '../cache/gpt2_low_high_arousal_0402b2024.json'\n",
    "with open(arousal_reps_path, 'r') as f:\n",
    "    arousal_reps = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any instances with 'negation' == True\n",
    "happy_sad_reps = [rep for rep in happy_sad_reps if not rep['negation']]\n",
    "tmp_hsr = happy_sad_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any instances with 'negation' == True\n",
    "arousal_reps = [rep for rep in arousal_reps if not rep['negation']]\n",
    "tmp_a = arousal_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp_hsr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m happy_sad_reps \u001b[38;5;241m=\u001b[39m \u001b[43mtmp_hsr\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp_hsr' is not defined"
     ]
    }
   ],
   "source": [
    "happy_sad_reps = tmp_hsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of happy sad reps 22860\n",
      "Keys in happy sad reps dict_keys(['final_prompt', 'final_prompt_ids', 'token_of_interest', 'prompt_template', 'note', 'negation', 'class_0_true', 'class_name', 'adjective', 'model', 'latent_space'])\n"
     ]
    }
   ],
   "source": [
    "# more info about the structure\n",
    "print(\"Length of happy sad reps\", len(happy_sad_reps)) # 3030(?)\n",
    "print(\"Keys in happy sad reps\", happy_sad_reps[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of arousal reps 13716\n",
      "Keys in arousal reps dict_keys(['final_prompt', 'final_prompt_ids', 'token_of_interest', 'prompt_template', 'note', 'negation', 'class_0_true', 'class_name', 'adjective', 'model', 'latent_space'])\n"
     ]
    }
   ],
   "source": [
    "# more info about the structure\n",
    "print(\"Length of arousal reps\", len(arousal_reps)) # 3030(?)\n",
    "print(\"Keys in arousal reps\", arousal_reps[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22860 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22860/22860 [00:10<00:00, 2228.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the latents \n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(happy_sad_reps))): \n",
    "    latent_vectors = happy_sad_reps[i]['latent_space']\n",
    "    flattened_vector = [val for layer in latent_vectors for head in layer for val in head]\n",
    "    happy_sad_reps[i]['flattened_latents'] = np.array(flattened_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13716/13716 [00:06<00:00, 2192.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Flatten the latents \n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(arousal_reps))): \n",
    "    latent_vectors = arousal_reps[i]['latent_space']\n",
    "    flattened_vector = [val for layer in latent_vectors for head in layer for val in head]\n",
    "    arousal_reps[i]['flattened_latents'] = np.array(flattened_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_sad_reps[0]['flattened_latents'] shape (9216,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-5.59991651])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"happy_sad_reps[0]['flattened_latents'] shape\", happy_sad_reps[0]['flattened_latents'].shape) # 24576\n",
    "# try inner product with valence_weight_vec\n",
    "happy_sad_reps[0]['flattened_latents'] @ valence_weight_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22860/22860 [00:00<00:00, 59134.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# dot product with valence_weight_vec, arousal_weight_vec\n",
    "for i in tqdm(range(len(happy_sad_reps))): \n",
    "    happy_sad_reps[i]['valence_score'] = (happy_sad_reps[i]['flattened_latents'] @ valence_weight_vec.T + valence_bias)[0]\n",
    "    happy_sad_reps[i]['arousal_score'] = (happy_sad_reps[i]['flattened_latents'] @ arousal_weight_vec.T + arousal_bias)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13716/13716 [00:00<00:00, 61758.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# dot product with valence_weight_vec, arousal_weight_vec\n",
    "for i in tqdm(range(len(arousal_reps))): \n",
    "    arousal_reps[i]['valence_score'] = (arousal_reps[i]['flattened_latents'] @ valence_weight_vec.T + valence_bias)[0]\n",
    "    arousal_reps[i]['arousal_score'] = (arousal_reps[i]['flattened_latents'] @ arousal_weight_vec.T + arousal_bias)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.717783185991082"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sad_reps[0]['valence_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.500227286332773"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arousal_reps[0]['valence_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valence_bad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sad_reps[0]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tense', 'excited', 'angry', 'delighted', 'frustrated', 'happy', 'depressed', 'content', 'bored', 'relaxed', 'tired', 'calm']\n",
      "HAS TIRED:  True\n"
     ]
    }
   ],
   "source": [
    "# plotly plot of valence and arousal scores, labels are happy_sad_reps[i]['adjective']. 2D plot, save to html\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "# arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "combined_reps = happy_sad_reps + arousal_reps  # This line is correct and should stay as is.\n",
    "\n",
    "valence_scores = -np.array([rep['valence_score'] for rep in combined_reps])\n",
    "arousal_scores = -np.array([rep['arousal_score'] for rep in combined_reps])\n",
    "highlight_mask = [rep['class_name'] == 'valence_bad' for rep in combined_reps]\n",
    "# highlight_mask = [False for rep in combined_reps]\n",
    "\n",
    "# Update the fig.add_trace() commands to include combined_reps logic\n",
    "# Specifically, ensure text_labels are generated for combined_reps\n",
    "adjectives_from_combined = {rep['adjective'].lower() for rep in combined_reps}  # Use set for O(1) lookup\n",
    "text_labels = [rep['adjective'] if rep['adjective'].lower() in words_of_interest and rep['adjective'].lower() in adjectives_from_combined else None for rep in combined_reps]\n",
    "\n",
    "# Then, use the updated `valence_scores`, `arousal_scores`, `highlight_mask`, and `text_labels` for your plotting logic.\n",
    "\n",
    "\n",
    "# highlight_mask = [rep['class_name'] == 'valence_bad' for rep in happy_sad_reps]\n",
    "valence_arousal_words = ['Tense',\n",
    "     'Excited',\n",
    "     'Angry',\n",
    "     'Delighted',\n",
    "     'Frustrated',\n",
    "     'Happy',\n",
    "     'Depressed',\n",
    "     'Content',\n",
    "     'Bored',\n",
    "     'Relaxed',\n",
    "     'Tired',\n",
    "     'Calm']\n",
    "valence_arousal_words = [word.lower() for word in valence_arousal_words]\n",
    "words_of_interest = valence_arousal_words\n",
    "print(words_of_interest)\n",
    "\n",
    "fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=valence_scores, y=arousal_scores, mode='markers', text=[rep['adjective'] for rep in happy_sad_reps]))\n",
    "# happy_sad_labels = [rep['class_name'] == 'valence_bad' for rep in happy_sad_reps]\n",
    "# fig.add_trace(go.Scatter(x=valence_scores, y=arousal_scores, mode='markers', text=[rep['adjective'] for rep in happy_sad_reps])\n",
    "\n",
    "# Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.6\n",
    "\n",
    "#marker_sizes = [50 if rep['adjective'].lower() in words_of_interest else 5 for rep in happy_sad_reps]\n",
    "marker_sizes = [100 if rep['adjective'].lower() in words_of_interest else 10 for rep in combined_reps]\n",
    "# text_labels = [rep['adjective'] if rep['adjective'].lower() in words_of_interest else None for rep in happy_sad_reps]\n",
    "\n",
    "# print(arousal_reps[0])\n",
    "# # Assuming arousal_reps is a list of dictionaries similar to the one printed\n",
    "has_tired = any(row.get('adjective') == 'bored' for row in combined_reps)\n",
    "\n",
    "print(\"HAS TIRED: \", has_tired)\n",
    "\n",
    "# print(happy_sad_reps)\n",
    "#text_labels = [rep['adjective'] if rep['adjective'].lower() in words_of_interest and rep['adjective'].lower() in adjectives_from_combined else None for rep in happy_sad_reps]\n",
    "\n",
    "fig.add_trace(go.Scatter(x=valence_scores[highlight_mask], y=arousal_scores[highlight_mask],\n",
    "                         mode='markers+text', text=[label for label, mask in zip(text_labels, highlight_mask) if mask],\n",
    "                         marker=dict(opacity=1.0, size=[size for size, mask in zip(marker_sizes, highlight_mask) if mask]), \n",
    "                         name='valence bad (label)'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=valence_scores[~np.array(highlight_mask)], y=arousal_scores[~np.array(highlight_mask)],\n",
    "                         mode='markers+text', text=[label for label, mask in zip(text_labels, highlight_mask) if not mask],\n",
    "                         marker=dict(opacity=marker_opacity, size=[size for size, mask in zip(marker_sizes, highlight_mask) if not mask]), \n",
    "                         name='valence good (label)'))\n",
    "\n",
    "\n",
    "\n",
    "# axis titles for x, y \n",
    "fig.update_xaxes(title_text='Valence Scores')\n",
    "fig.update_yaxes(title_text='Arousal Scores')\n",
    "\n",
    "# color by happy or sad\n",
    "\n",
    "fig.update_layout(title='Valence and Arousal Scores over ~250 prompt templates, 190 adjectives, cache/gpt2_happy_sad_0330b2024.json, axes from cache/happy_sad_0330b2024/weights.npz and cache/low_high_arousal_0330b2024/weights.npz')\n",
    "fig.write_html('r7vised_valence_arousal_scores.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make another plot like the one above where we color by the prompt template used. \n",
    "we will need to get the unique values of `happy_sad_reps[i]['prompt_template']`. \n",
    "\n",
    "Since there are so many, we don't want to label them with a `name` in the \n",
    "fig.add_trace() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "\n",
    "# Get the unique prompt templates\n",
    "prompt_templates = list(set([rep['prompt_template'] for rep in happy_sad_reps]))\n",
    "\n",
    "# Create a color map for the prompt templates\n",
    "color_map = {template: f'rgb({np.random.randint(0, 256)}, {np.random.randint(0, 256)}, {np.random.randint(0, 256)})' for template in prompt_templates}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.6\n",
    "\n",
    "# Create traces for each prompt template\n",
    "for template in prompt_templates:\n",
    "    template_mask = [rep['prompt_template'] == template for rep in happy_sad_reps]\n",
    "    fig.add_trace(go.Scatter(x=valence_scores[template_mask], y=arousal_scores[template_mask],\n",
    "                             mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask],\n",
    "                             marker=dict(color=color_map[template], opacity=marker_opacity)))\n",
    "\n",
    "# Axis titles for x, y\n",
    "fig.update_xaxes(title_text='Valence Scores')\n",
    "fig.update_yaxes(title_text='Arousal Scores')\n",
    "\n",
    "# Color by prompt template\n",
    "fig.update_layout(title='Valence and Arousal Scores for Happy Sad Adjectives (Colored by Prompt Template)')\n",
    "\n",
    "fig.write_html('pr0mpt_colored_valence_arousal_scores_prompt_template.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One plot per prompt to make it less confusing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m template_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m template \u001b[38;5;28;01mfor\u001b[39;00m rep \u001b[38;5;129;01min\u001b[39;00m combined_reps])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Recalculate highlight_mask to work within the filtered set by template_mask\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcombined_reps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtemplate_mask\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m highlight_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([((rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow_arousal_adjectives\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_bad\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m rep \u001b[38;5;129;01min\u001b[39;00m combined_reps])[template_mask]\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Create a subdirectory for the plots\n",
    "output_dir = 'prompt_template_plots_20240402f'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "# arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "\n",
    "# # Get the unique prompt templates\n",
    "# prompt_templates = list(set([rep['prompt_template'] for rep in happy_sad_reps]))\n",
    "prompt_templates = list(set([rep['prompt_template'] for rep in combined_reps]))\n",
    "\n",
    "# # Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.8\n",
    "\n",
    "for template in prompt_templates:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Assume `template` is defined, and then create `template_mask`\n",
    "    template_mask = np.array([rep['prompt_template'] == template for rep in combined_reps])\n",
    "\n",
    "    # Recalculate highlight_mask to work within the filtered set by template_mask\n",
    "    print(np.array(combined_reps)[template_mask])\n",
    "    break\n",
    "    highlight_mask = np.array([((rep['class_name'] == 'low_arousal_adjectives') or (rep['class_name'] == 'valence_bad')) for rep in combined_reps])[template_mask]\n",
    "\n",
    "    # Apply template_mask to scores and labels\n",
    "    valence_scores_filtered = valence_scores[template_mask]\n",
    "    arousal_scores_filtered = arousal_scores[template_mask]\n",
    "    text_labels_filtered = np.array(text_labels)[template_mask]\n",
    "\n",
    "    # Then for plotting, use the filtered scores and labels, applying highlight_mask directly\n",
    "    fig.add_trace(go.Scatter(x=valence_scores_filtered[highlight_mask], y=arousal_scores_filtered[highlight_mask],\n",
    "                            mode='markers+text', text=[label for label, is_highlighted in zip(text_labels_filtered, highlight_mask) if is_highlighted],\n",
    "                            marker=dict(opacity=1.0, size=[size for size, is_highlighted in zip(marker_sizes, highlight_mask) if is_highlighted]), \n",
    "                            name='valence bad (label)'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valence_scores_filtered[~highlight_mask], y=arousal_scores_filtered[~highlight_mask],\n",
    "                            mode='markers+text', text=[label for label, is_highlighted in zip(text_labels_filtered, highlight_mask) if not is_highlighted],\n",
    "                            marker=dict(opacity=0.6, size=[size for size, is_highlighted in zip(marker_sizes, highlight_mask) if not is_highlighted]), \n",
    "                            name='valence good (label)'))\n",
    "\n",
    "\n",
    "    # Create separate traces for highlighted and non-highlighted elements\n",
    "    # fig.add_trace(go.Scatter(x=valence_scores[template_mask][highlight_mask], y=arousal_scores[template_mask][highlight_mask],\n",
    "    #                          mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask and rep['class_name'] == 'valence_bad'],\n",
    "    #                          marker=dict(opacity=1.0),\n",
    "    #                          name='valence bad'))\n",
    "\n",
    "    # fig.add_trace(go.Scatter(x=valence_scores[template_mask][~np.array(highlight_mask)], y=arousal_scores[template_mask][~np.array(highlight_mask)],\n",
    "    #                          mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask and rep['class_name'] != 'valence_bad'],\n",
    "    #                          marker=dict(opacity=marker_opacity),\n",
    "    #                          name='valence good'))\n",
    "\n",
    "    # Axis titles for x, y\n",
    "    fig.update_xaxes(title_text='Valence')\n",
    "    fig.update_yaxes(title_text='Arousal')\n",
    "\n",
    "    text_labels_short = [x for x in text_labels if x is not None]\n",
    "    print(list(reversed(text_labels_short)))\n",
    "    print()\n",
    "\n",
    "    # Set the plot title to include the prompt template\n",
    "    #fig.update_layout(title=f'Valence and Arousal Scores for Prompt Template: {template}')\n",
    "\n",
    "    # Update the title text size\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f'Valence and Arousal Scores for Prompt Template: {template}', #'Valence and Arousal Scores over ~250 prompt templates, 190 adjectives, cache/gpt2_happy_sad_0330b2024.json, axes from cache/happy_sad_0330b2024/weights.npz and cache/low_high_arousal_0330b2024/weights.npz',\n",
    "            #'y':0.9,\n",
    "            #'x':0.5,\n",
    "            #'xanchor': 'center',\n",
    "            #'yanchor': 'top',\n",
    "            'font': {'size': 32}  # Increase the font size as needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Update legend text size\n",
    "    fig.update_layout(\n",
    "        legend={'font': {'size': 28}}  # Increase the font size as needed\n",
    "    )\n",
    "\n",
    "    # Update axis titles text size\n",
    "    fig.update_xaxes(title_font={'size': 28})  # Increase the font size as needed\n",
    "    fig.update_yaxes(title_font={'size': 28})  # Increase the font size as needed\n",
    "\n",
    "    # Generate a filename based on the prompt template\n",
    "    filename = f\"{output_dir}/valence_arousal_scores_{template.replace(' ', '_')}.html\"\n",
    "    fig.write_html(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
