{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tijuana Results -- March 29, 2024\n",
    "\n",
    "Git: `2d5e2612b485b10055834a38af6a16dfa8f1dfad`\n",
    "\n",
    "To generate the cache/gpt2_happy_sad_03292024.json (~2500 prompt-adjective pairs\n",
    "with reps), I ran this command. \n",
    "\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03292024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_03292024.json \n",
    "```\n",
    "\n",
    "To run the grok script, we'll run this command: \n",
    "\n",
    "```\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5\n",
    "```\n",
    "Results: \n",
    "The weights of the linear regression model trained on `happy_sad_adjectives.json` \n",
    "subbed into `prompt_templates_03292024.json` for predicting the valence \n",
    "(binary, +/-) can be found here: \n",
    "\n",
    "\n",
    "## Arousal Axis Discrimination Experiment\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/low_high_arousal_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03302024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_low_high_arousal_03302024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_low_high_arousal_03302024.json \\\n",
    "    --output-dir cache/arousal_results/\n",
    "```\n",
    "\n",
    "## Another Round of Valence Axis Discrimination\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_03302024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_03302024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_happy_sad_03302024.json \\\n",
    "    --output-dir cache/happy_sad_03302024/\n",
    "```\n",
    "\n",
    "\n",
    "## 30 -> 254 Prompt Templates\n",
    "New dataset of prompts in `datasets/prompt_templates_0330b2024.json`\n",
    "### Valence Axis (254 Prompt Templates)\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/happy_sad_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_0330b2024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_happy_sad_0330b2024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_happy_sad_0330b2024.json \\\n",
    "    --output-dir cache/happy_sad_0330b2024/\n",
    "```\n",
    "\n",
    "### Arousal Axis (254 Prompt Templates)\n",
    "```bash\n",
    "python3 scripts/get_value_reps.py \\\n",
    "    --adjective_json datasets/low_high_arousal_adjectives.json \\\n",
    "    --prompt_templates datasets/prompt_templates_0330b2024.json \\\n",
    "    --model_name gpt2 \\\n",
    "    --out_path cache/gpt2_low_high_arousal_0330b2024.json \n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/grok_intrinsic_geometry.py \\\n",
    "    --plot-lr \\\n",
    "    --plot-all \\\n",
    "    --pca-components 20 \\\n",
    "    --knn-clusters 5 \\\n",
    "    --dataset-json cache/gpt2_low_high_arousal_0330b2024.json \\\n",
    "    --output-dir cache/low_high_arousal_0330b2024/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of happy sad adjectives 2\n",
      "Length of arousal adjectives 2\n",
      "Length of templates march 29th 14\n",
      "Length of templates march 30th 30\n",
      "Length of templates march 30th 254\n"
     ]
    }
   ],
   "source": [
    "happy_sad_adjective_path = '../datasets/happy_sad_adjectives.json'\n",
    "# load json, print length\n",
    "import json\n",
    "with open(happy_sad_adjective_path, 'r') as f:\n",
    "    happy_sad_adjectives = json.load(f)\n",
    "print(\"Length of happy sad adjectives\", len(happy_sad_adjectives)) # 1000\n",
    "\n",
    "\n",
    "# Sanity check on the data \n",
    "arousal_adj_path = '../datasets/low_high_arousal_adjectives.json'\n",
    "# load json, print length \n",
    "with open(arousal_adj_path, 'r') as f:\n",
    "    arousal_adj = json.load(f)\n",
    "print(\"Length of arousal adjectives\", len(arousal_adj)) # 1000\n",
    "\n",
    "templates_29_path = '../datasets/prompt_templates_03292024.json'\n",
    "# load json, print length\n",
    "with open(templates_29_path, 'r') as f:\n",
    "    templates_29 = json.load(f)\n",
    "\n",
    "print(\"Length of templates march 29th\", len(templates_29)) # 1000\n",
    "\n",
    "templates_30_path = '../datasets/prompt_templates_03302024.json'\n",
    "# load json, print length\n",
    "with open(templates_30_path, 'r') as f:\n",
    "    templates_30 = json.load(f)\n",
    "print(\"Length of templates march 30th\", len(templates_30)) # 1000\n",
    "\n",
    "templates_30b_path = '../datasets/prompt_templates_0330b2024.json'\n",
    "with open(templates_30b_path, 'r') as f:\n",
    "    templates_30b = json.load(f)\n",
    "print(\"Length of templates march 30th\", len(templates_30b)) # 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence-Arousal Axes Hack\n",
    "\n",
    "Let us use the learned weights from `cache/happy_sad_0330b2024/weights.npz` \n",
    "and `cache/low_high_arousal_0330b2024/weights.npz` along with the activations \n",
    "from `cache/gpt2_happy_sad_0330b2024.json` to make a valence-arousal axis. \n",
    " 1. Load weights for valence (happy_sad) and arousal (low_high_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence weight vec shape (1, 9216)\n",
      "Valence bias shape (1,)\n",
      "Arousal weight vec shape (1, 9216)\n",
      "Arousal bias shape (1,)\n"
     ]
    }
   ],
   "source": [
    "valence_weight_path = '../cache/happy_sad_0330b2024/weights.npz'\n",
    "arousal_weight_path = '../cache/low_high_arousal_0330b2024/weights.npz'\n",
    "\n",
    "import numpy as np\n",
    "valence_weights = np.load(valence_weight_path)\n",
    "arousal_weights = np.load(arousal_weight_path)\n",
    "\n",
    "valence_weight_vec = valence_weights['arr_0']\n",
    "valence_bias = valence_weights['arr_1']\n",
    "\n",
    "arousal_weight_vec = arousal_weights['arr_0']\n",
    "arousal_bias = arousal_weights['arr_1']\n",
    "\n",
    "print(\"Valence weight vec shape\", valence_weight_vec.shape) # (1000, 768)\n",
    "print(\"Valence bias shape\", valence_bias.shape) # (1000,)\n",
    "\n",
    "print(\"Arousal weight vec shape\", arousal_weight_vec.shape) # (1000, 768)\n",
    "print(\"Arousal bias shape\", arousal_bias.shape) # (1000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the latent reps from cache/gpt2_happy_sad_0330b2024.json\n",
    "happy_sad_reps_path = '../cache/gpt2_happy_sad_0330b2024.json'\n",
    "with open(happy_sad_reps_path, 'r') as f:\n",
    "    happy_sad_reps = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any instances with 'negation' == True\n",
    "happy_sad_reps = [rep for rep in happy_sad_reps if not rep['negation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of happy sad reps 24130\n",
      "Keys in happy sad reps dict_keys(['final_prompt', 'final_prompt_ids', 'token_of_interest', 'prompt_template', 'note', 'negation', 'class_0_true', 'class_name', 'adjective', 'model', 'latent_space'])\n"
     ]
    }
   ],
   "source": [
    "# more info about the structure\n",
    "print(\"Length of happy sad reps\", len(happy_sad_reps)) # 3030(?)\n",
    "print(\"Keys in happy sad reps\", happy_sad_reps[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now flatten the reps (as in `scripts/grok_intrinsic_geometry.py`) and \n",
    "get the dot-product with the valence weight vector (`valence_weight_vec`) \n",
    "and the arousal weight vector `arousal_weight_vec`. \n",
    "\n",
    "Once this is done, each element in `happy_sad_reps` will be associated with a \n",
    "\"valence score\" and an \"arousal score\". We will then be able to make a scatter\n",
    "plot. \n",
    "\n",
    "happy_sad_reps[i]['latent_space'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24130/24130 [00:09<00:00, 2445.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the latents \n",
    "for i in tqdm(range(len(happy_sad_reps))): \n",
    "    latent_vectors = happy_sad_reps[i]['latent_space']\n",
    "    flattened_vector = [val for layer in latent_vectors for head in layer for val in head]\n",
    "    happy_sad_reps[i]['flattened_latents'] =np.array(flattened_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_sad_reps[0]['flattened_latents'] shape (9216,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.18753808])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"happy_sad_reps[0]['flattened_latents'] shape\", happy_sad_reps[0]['flattened_latents'].shape) # 24576\n",
    "# try inner product with valence_weight_vec\n",
    "happy_sad_reps[0]['flattened_latents'] @ valence_weight_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6891/24130 [00:00<00:00, 68898.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24130/24130 [00:00<00:00, 75511.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# dot product with valence_weight_vec, arousal_weight_vec\n",
    "for i in tqdm(range(len(happy_sad_reps))): \n",
    "    happy_sad_reps[i]['valence_score'] = (happy_sad_reps[i]['flattened_latents'] @ valence_weight_vec.T + valence_bias)[0]\n",
    "    happy_sad_reps[i]['arousal_score'] = (happy_sad_reps[i]['flattened_latents'] @ arousal_weight_vec.T + arousal_bias)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0767269533566157"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sad_reps[0]['valence_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valence_bad'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sad_reps[0]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot of valence and arousal scores, labels are happy_sad_reps[i]['adjective']. 2D plot, save to html\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "\n",
    "highlight_mask = [rep['class_name'] == 'valence_bad' for rep in happy_sad_reps]\n",
    "\n",
    "fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=valence_scores, y=arousal_scores, mode='markers', text=[rep['adjective'] for rep in happy_sad_reps]))\n",
    "# happy_sad_labels = [rep['class_name'] == 'valence_bad' for rep in happy_sad_reps]\n",
    "# fig.add_trace(go.Scatter(x=valence_scores, y=arousal_scores, mode='markers', text=[rep['adjective'] for rep in happy_sad_reps])\n",
    "\n",
    "# Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.6\n",
    "\n",
    "# Create separate traces for highlighted and non-highlighted elements\n",
    "fig.add_trace(go.Scatter(x=valence_scores[highlight_mask], y=arousal_scores[highlight_mask],\n",
    "                         mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, highlight_mask) if mask],\n",
    "                         marker=dict(opacity=1.0), \n",
    "                         name='valence bad (label)'))  # Highlighted elements have an opacity of 1.0\n",
    "\n",
    "fig.add_trace(go.Scatter(x=valence_scores[~np.array(highlight_mask)], y=arousal_scores[~np.array(highlight_mask)],\n",
    "                         mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, highlight_mask) if not mask],\n",
    "                         marker=dict(opacity=marker_opacity), \n",
    "                         name='valence good (label)'))  # Non-highlighted elements have the specified opacity\n",
    "\n",
    "\n",
    "# axis titles for x, y \n",
    "fig.update_xaxes(title_text='Valence Scores')\n",
    "fig.update_yaxes(title_text='Arousal Scores')\n",
    "\n",
    "# color by happy or sad\n",
    "\n",
    "fig.update_layout(title='Valence and Arousal Scores over ~250 prompt templates, 190 adjectives, cache/gpt2_happy_sad_0330b2024.json, axes from cache/happy_sad_0330b2024/weights.npz and cache/low_high_arousal_0330b2024/weights.npz')\n",
    "fig.write_html('r7vised_valence_arousal_scores.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make another plot like the one above where we color by the prompt template used. \n",
    "we will need to get the unique values of `happy_sad_reps[i]['prompt_template']`. \n",
    "\n",
    "Since there are so many, we don't want to label them with a `name` in the \n",
    "fig.add_trace() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "\n",
    "# Get the unique prompt templates\n",
    "prompt_templates = list(set([rep['prompt_template'] for rep in happy_sad_reps]))\n",
    "\n",
    "# Create a color map for the prompt templates\n",
    "color_map = {template: f'rgb({np.random.randint(0, 256)}, {np.random.randint(0, 256)}, {np.random.randint(0, 256)})' for template in prompt_templates}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.6\n",
    "\n",
    "# Create traces for each prompt template\n",
    "for template in prompt_templates:\n",
    "    template_mask = [rep['prompt_template'] == template for rep in happy_sad_reps]\n",
    "    fig.add_trace(go.Scatter(x=valence_scores[template_mask], y=arousal_scores[template_mask],\n",
    "                             mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask],\n",
    "                             marker=dict(color=color_map[template], opacity=marker_opacity)))\n",
    "\n",
    "# Axis titles for x, y\n",
    "fig.update_xaxes(title_text='Valence Scores')\n",
    "fig.update_yaxes(title_text='Arousal Scores')\n",
    "\n",
    "# Color by prompt template\n",
    "fig.update_layout(title='Valence and Arousal Scores for Happy Sad Adjectives (Colored by Prompt Template)')\n",
    "\n",
    "fig.write_html('pr0mpt_colored_valence_arousal_scores_prompt_template.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One plot per prompt to make it less confusing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Create a subdirectory for the plots\n",
    "output_dir = 'prompt_template_plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "valence_scores = -np.array([rep['valence_score'] for rep in happy_sad_reps])\n",
    "arousal_scores = -np.array([rep['arousal_score'] for rep in happy_sad_reps])\n",
    "\n",
    "# Get the unique prompt templates\n",
    "prompt_templates = list(set([rep['prompt_template'] for rep in happy_sad_reps]))\n",
    "\n",
    "# Set the alpha value for all elements to 0.3\n",
    "marker_opacity = 0.8\n",
    "\n",
    "for template in prompt_templates:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create a mask for the elements corresponding to the current prompt template\n",
    "    template_mask = [rep['prompt_template'] == template for rep in happy_sad_reps]\n",
    "\n",
    "    # Create a mask for the elements to highlight (valence bad)\n",
    "    highlight_mask = [rep['class_name'] == 'valence_bad' for rep, mask in zip(happy_sad_reps, template_mask) if mask]\n",
    "\n",
    "    # Create separate traces for highlighted and non-highlighted elements\n",
    "    fig.add_trace(go.Scatter(x=valence_scores[template_mask][highlight_mask], y=arousal_scores[template_mask][highlight_mask],\n",
    "                             mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask and rep['class_name'] == 'valence_bad'],\n",
    "                             marker=dict(opacity=1.0),\n",
    "                             name='valence bad'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valence_scores[template_mask][~np.array(highlight_mask)], y=arousal_scores[template_mask][~np.array(highlight_mask)],\n",
    "                             mode='markers', text=[rep['adjective'] for rep, mask in zip(happy_sad_reps, template_mask) if mask and rep['class_name'] != 'valence_bad'],\n",
    "                             marker=dict(opacity=marker_opacity),\n",
    "                             name='valence good'))\n",
    "\n",
    "    # Axis titles for x, y\n",
    "    fig.update_xaxes(title_text='Valence Scores')\n",
    "    fig.update_yaxes(title_text='Arousal Scores')\n",
    "\n",
    "    # Set the plot title to include the prompt template\n",
    "    fig.update_layout(title=f'Valence and Arousal Scores for Prompt Template: {template}')\n",
    "\n",
    "    # Generate a filename based on the prompt template\n",
    "    filename = f\"{output_dir}/valence_arousal_scores_{template.replace(' ', '_')}.html\"\n",
    "    fig.write_html(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
